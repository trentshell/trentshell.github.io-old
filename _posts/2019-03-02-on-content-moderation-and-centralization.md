---
layout: post
title: "On Content Moderation and Centralization"
date: "2019-03-02"
categories: tech facebook 
---

Casey Newton at The Verge wrote a lengthy piece last week entitled "[The Secret Lives of Facebook Moderators in America](https://www.theverge.com/2019/2/25/18229714/cognizant-facebook-content-moderator-interviews-trauma-working-conditions-arizona)" which documents the horrors that content moderators face on a daily basis. It's worth reading if for nothing else than gaining an understanding of how traumatic and serious the job is. Be warned that it contains descriptions of the real things these people face: the worst side of humanity posted to the world's biggest social network.

> She presses play. The video depicts a man being murdered. Someone is stabbing him, dozens of times, while he screams and begs for his life. Chloe’s job is to tell the room whether this post should be removed. She knows that section 13 of the Facebook community standards prohibits videos that depict the murder of one or more people. When Chloe explains this to the class, she hears her voice shaking.

The article details the nature of the truly awful labor conditions imposed by the services vendor Cognizant. While such near-criminal policies are inexcusable, it doesn't surprise me that consistent exposure to such depraved content will eventually result in such an appalling environment. Since reading the piece (and earlier this year listening to a [Radiolab podcast](https://www.wnycstudios.org/story/post-no-evil) on the same subject) I've been thinking more about the nature of centralized social networks and whether such content moderation is an inherent byproduct or simply an economic solution. Facebook is not alone in its use of dedicated (and often outsourced) content moderation teams but is highlighted here due to its status as the largest site.

I also appreciated [John Gruber's take](https://daringfireball.net/2019/03/life_as_a_facebook_moderator) on this piece, in which he also commented on this unsettling practice:

> The bottom line: If this is what it takes to moderate Facebook, it’s an indictment of the basic concept of Facebook itself. In theory it sounds like a noble idea to let everyone in the world post whatever they want and have it be connected and amplified to like-minded individuals… In practice, it’s a disaster.

Basic economies of labor dictate that centralized problems require centralized solutions, and that specialization will win out in the end. Decentralized platforms like blogs, web forums, and podcasts all allowed content moderation to be dealt with at a scale that normal people can deal with. To have your full-time occupation be to sift through the worst of 2 billion people is unprecedented and dehumanizing.

While some might argue that machine learning will eventually help to alleviate these issues, I would argue that automation will also increase the amount of collective garbage that these systems would need to recognize, furthering the need for human intervention and training. I don't think more centralization (which is a necessity for machine learning) is the answer here. Furthermore, I don't know if there truly is an answer. I tend to think that this is a Band-Aid on a genetic disorder. Maybe we will reach a point where a social network is unsustainable due to the lack of willing victims to clean up the mess it leaves behind. Gruber finishes on this note:

> There is something fundamentally wrong with a platform that — while operating exactly as designed — requires thousands of employees to crush their own souls.

Speaking for myself, it makes me even more wary of using centralized social networks. Sometimes there isn't a feasible alternative, but awareness of a problem is the first step toward addressing it.
